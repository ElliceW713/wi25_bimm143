---
title: "Class 7: Machine Learning 1"
author: "Ellice Wang (PID: A16882742)"
date: January 28, 2025
format: pdf
---

Today we will explore unsupervised machine learning methods including clustering & dimensionality reduction methods. 

Let's start by making up some data (where we know there are clear groups) that we can use to test out different clustering methods.

We can use 'rnorm()' function to help us here:

```{r}
hist(rnorm(n=3000, mean = 3))
```

Make data with two "clusters"

```{r}
x <- c(rnorm(30, mean=-3), rnorm(30, mean=+3))

z <- cbind(x=x, y=rev(x))
head(z)

plot(z)
```

## K-means clustering

The main function in "base" R for K-means clustering is called 'kmeans()' 

```{r}
k <- kmeans(z, center = 2) 
k
```

```{r}
attributes(k)
```

> Q. How many points lie in each cluster?

```{r}
k$size
```

> Q. What component of our results tells us about the cluster membership (i.e. which point lives in which cluster)?

```{r}
k$cluster
```

>Q. Center of each cluster?

```{r}
k$centers
```

> Q. Put this result info together and make a little "base R" plot of our clustering result. Also add the cluster center points to this plot. 

```{r}
plot(z, col=(k$cluster))
points(k$centers, col="blue", pch=15, cex=2)
```

> Q. Run kmeans on our input 'z' and define 4 clusters

```{r}
four_cluster <- kmeans(z, centers=4)
plot(z, col=four_cluster$cluster)
points(four_cluster$centers, col="darkgrey", pch=15, cex=1.5)
```

```{r}
# total sum of squares which tells us how spread out it is 
# the smaller the totss, the better the visualization
four_cluster$totss
```

## Hierarchical Clustering

The main function in base R for this called 'hclust()'. It will take as input a distance matrix (key point is that you can't just give your "raw" data as input - must first calculate a distance matrix from data)

```{r}
d <- dist(z)
hc <- hclust(d)
hc
```

```{r}
plot(hc)
abline(h=10, col="red")
```

Once I inspect the "tree" I can "cut" the tree to yield my groupings or clusters. The function to this is called 'cutree()'

```{r}
grps<- cutree(hc, h=10)
```

```{r}
plot(z, col=grps)
```
## Hands on with Principle Component Analysis (PCA)

Let's examine some 17-dimensional data. 

```{r}
# load in dataframe of UK foods
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)

# explore the data
dim(x)
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions? \
There are 17 rows and 5 columns in the data frame. I used 'dim()' to answer this question, but 'nrow()' and 'ncol()' would also work. 

```{r}
# preview first 6 rows
head(x)
```

```{r}
# removes first column names

rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

```{r}
# check dimensions of edited dataframe
dim(x)
```

```{r}
# alternative way to edit dataframe

x <- read.csv(url, row.names=1)
head(x)
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances? \
I prefer using the method that sets the 'row.names' argument when loading in the dataframe. It is a more streamlined way to input data. I would use it after checking what the data looked like and determining whether it should be used. Setting 'x <- x[,-1]' multiple times will remove the first column multiple times. 

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

```{r}
test <- as.matrix(x)
color <- rainbow(nrow(x))
test <- cbind(test, color)
test
```


> Q3: Changing what optional argument in the above barplot() function results in the following plot? \
instead of 'beside=T' I set 'beside=F' to generate the stacked barplot

> Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot? \
The result is a matrix of scatterplots of the dataframe comparing the amount of different food groups each country eats. If a given point is on the diagonal for a given plot, it means that the two countries each similar amounts of that food. For example, England vs Wales plots are in the 2x1 and 1x2 positions. The blue dot lies on the diagonal for these two countries, indicating that England and Wales consume similar amounts of the food represented by the blue dot. 

```{r}
pairs(x, col=rainbow(10), pch=16)
```
> Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set? \
Northern Ireland's biggest difference is their consumption of the foods represented by the orange and blue dots. Orange represents carcass meat and the blue represents fresh fruit. 

Looking at these types of "pairwise plots" can be helpful but it does not scale well & kind of sucks...

### PCA to the rescue!

The main function for PCA in base R is called 'prcomp()'. This function wants the transpose of our input data - i.e. the important foods in as columns and the countries as rows.

```{r}
# transpose data
head(t(x))
```

```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
```
Let's see what is in our PCA results

```{r}
attributes(pca)
```
The 'pca$x' result object is where we will focus first as this details how the countries are related to each other in terms of our new "axis"(a.k.a. PCs). 

```{r}
pca$x[,2]
```


```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col=c("orange", "red", "blue", 
                                              "darkgreen"))
```

We can look at the so-called PC "loadings" result to see how the original foods contribute to our new PCs (i.e. how the original variables contribute to our new better variables)

```{r}
pca$rotation
```

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

> Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about? \
Fresh potatoes and soft drinks feature prominantly (similar to PC1). The other food groups feature less prominantly than in the first loading graph. PC2 is the vector that is perpendicular to PC1 and tells us about the second largest source of variation in the data not captured by PC1. 

```{r}
# loading plot for PC2
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```

```{r}
library(ggplot2)

df <- as.data.frame(pca$x)
df_lab <- tibble::rownames_to_column(df, "Country")

# Our first basic plot
ggplot(df_lab) + 
  aes(PC1, PC2, col=Country) + 
  geom_point()
```

Nicer looking ggplot

```{r}
ggplot(df_lab) + 
  aes(PC1, PC2, col=Country, label=Country) + 
  geom_hline(yintercept = 0, col="gray") +
  geom_vline(xintercept = 0, col="gray") +
  geom_point(show.legend = FALSE) +
  geom_label(hjust=1, nudge_x = -10, show.legend = FALSE) +
  expand_limits(x = c(-300,500)) +
  xlab("PC1 (67.4%)") +
  ylab("PC2 (28%)") +
  theme_bw()
```

loading plots

```{r}
ld <- as.data.frame(pca$rotation)
ld_lab <- tibble::rownames_to_column(ld, "Food")

ggplot(ld_lab) +
  aes(PC1, Food) +
  geom_col() 
```

Nicer looking loading plot

```{r}
ggplot(ld_lab) +
  aes(PC1, reorder(Food, PC1), bg=PC1) +
  geom_col() + 
  xlab("PC1 Loadings/Contributions") +
  ylab("Food Group") +
  scale_fill_gradient2(low="purple", mid="gray", high="darkgreen", guide=NULL) +
  theme_bw()
```

Biplots are another way to visualize the information. 

```{r}
biplot(pca)
```

## PCA of RNA-seq data

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

> Q10: How many genes and samples are in this data set? \
There are 100 genes and 10 samples in this dataset. 

```{r}
dim(rna.data)
```

Plot a PCA graph of the RNA-seq data

```{r}
## Again we have to take the transpose of our data 
pca <- prcomp(t(rna.data), scale=TRUE)
 
## Simple un polished plot of pc1 and pc2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")
```

```{r}
summary(pca)
```

Using ggplot 

```{r}
library(ggplot2)

## Variance captured per PC 
pca.var <- pca$sdev^2

## Percent variance is often more informative to look at 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

df <- as.data.frame(pca$x)

# Add a 'wt' and 'ko' "condition" column to our plot
df$samples <- colnames(rna.data) 
df$condition <- substr(colnames(rna.data),1,2)

p <- ggplot(df) + 
        aes(PC1, PC2, label=samples, col=condition) + 
        geom_label(show.legend = FALSE) +
# add titles and labels and change theme to make graph look nicer 
        labs(title="PCA of RNASeq Data",
         subtitle = "PC1 clealy seperates wild-type from knock-out samples",
         x=paste0("PC1 (", pca.var.per[1], "%)"),
         y=paste0("PC2 (", pca.var.per[2], "%)"),
         caption="Class example data") +
         theme_bw()
p
```